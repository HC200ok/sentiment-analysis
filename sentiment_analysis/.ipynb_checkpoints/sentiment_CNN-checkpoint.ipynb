{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17400, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "list = []\n",
    "size = 2900\n",
    "df = []\n",
    "\n",
    "emotions = [\"happy\", \"sad\", \"disgust\", \"angry\", \"fear\", \"surprise\"]\n",
    "\n",
    "for i, es in enumerate(emotions):\n",
    "  data = shuffle(pd.read_json(\"./data/processed/{}.json\".format(es))).iloc[:int(size)]\n",
    "  data['label'] = i\n",
    "  df.append(data)\n",
    "\n",
    "df = pd.concat(df)\n",
    "df = shuffle(df)\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "df.shape  #不是数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9787, 100)\n",
      "(3263, 100)\n",
      "(4350, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_features = 2000\n",
    "maxlen = 100\n",
    "\n",
    "# 特征化：\n",
    "# 1. text的特征化   Tokenizer()\n",
    "# 2. label的特征化  to_categorical()\n",
    "# label: 1 => [0,1,0,0,0,0]\n",
    "y = to_categorical(y)\n",
    "\n",
    "# 切割测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# 用keras的Tokenizer进行文本预处理，序列化，向量化等\n",
    "# 简单讲解Tokenizer如何实现转换。当我们创建了一个Tokenizer对象后，使用该对象的fit_on_texts()函数，可以将输入的文本中的每个词编号，编号是根据词频的，词频越大，编号越小。可能这时会有疑问：Tokenizer是如何判断文本的一个词呢？其实它是以空格去识别每个词。因为英文的词与词之间是以空格分隔，所以我们可以直接将文本作为函数的参数，但是当我们处理中文文本时，我们需要使用分词工具将词与词分开，并且词间使用空格分开。具体实现如下：\n",
    "def preprocess(data, tokenizer, maxlen = maxlen):\n",
    "    return(pad_sequences(tokenizer.texts_to_sequences(data), maxlen = maxlen))\n",
    "tokenizer = Tokenizer(num_words = max_features, char_level = True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = preprocess(X_train, tokenizer, maxlen)\n",
    "X_test = preprocess(X_test, tokenizer, maxlen)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)\n",
    "X_train.shape, X_val.shape, X_test.shape\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "# 使用Sequential模型来实现极为方便\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 150, input_length = maxlen))\n",
    "\n",
    "# Dropout 层还是非常重要的，值得一提。 Dropout 是一种方法能正则化我们的模型，防止出现过拟合。\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# 卷积过滤器的数量、每个卷积内核中的行数、每个卷积内核中的列数。\n",
    "model.add(Conv1D(32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "\n",
    "# MaxPooling2D 是一种减少模型中参数的方法，方式是在之前的层上滑动一个 2x2 的池化过滤器，在 2x2 过滤器中取 4 个值的最大值\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "\n",
    "# 卷积层(Conv Layer)\n",
    "model.add(Conv1D(64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "\n",
    "# 池化层(Pooling Layer)\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "\n",
    "# 最后再输出之前我们需要将上述几个层的输出作为全连接层的输入。由于卷积层和池化层的输出是2D的，因此需要将其压平，此时需要用到Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# 最后一层的输出大小为 6，对应了情感的 6 个类别。\n",
    "# 全连接层(Dense Layer)\n",
    "# 如果分类的类别过多地话则可以用softmax作为激活函数)\n",
    "model.add(Dense(6, activation = 'softmax'))\n",
    "\n",
    "# 现在只需编译模型，就可以训练它了。在我们编译模型时，会声明损失函数和优化器（SGD，Adam 等等）。\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9787 samples, validate on 3263 samples\n",
      "Epoch 1/15\n",
      "9787/9787 [==============================] - 25s 3ms/step - loss: 1.7778 - acc: 0.1998 - val_loss: 1.7621 - val_acc: 0.2164\n",
      "Epoch 2/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.7395 - acc: 0.2473 - val_loss: 1.7205 - val_acc: 0.2568\n",
      "Epoch 3/15\n",
      "9787/9787 [==============================] - 21s 2ms/step - loss: 1.6888 - acc: 0.2849 - val_loss: 1.6907 - val_acc: 0.2593\n",
      "Epoch 4/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.6491 - acc: 0.3125 - val_loss: 1.6754 - val_acc: 0.2798\n",
      "Epoch 5/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.6121 - acc: 0.3380 - val_loss: 1.6554 - val_acc: 0.3040\n",
      "Epoch 6/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.5870 - acc: 0.3593 - val_loss: 1.6517 - val_acc: 0.2905\n",
      "Epoch 7/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.5515 - acc: 0.3840 - val_loss: 1.6504 - val_acc: 0.3101\n",
      "Epoch 8/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.5159 - acc: 0.4060 - val_loss: 1.6337 - val_acc: 0.3092\n",
      "Epoch 9/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.4847 - acc: 0.4252 - val_loss: 1.6512 - val_acc: 0.3157\n",
      "Epoch 10/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.4479 - acc: 0.4472 - val_loss: 1.6388 - val_acc: 0.3255\n",
      "Epoch 11/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.4147 - acc: 0.4643 - val_loss: 1.6642 - val_acc: 0.3160\n",
      "Epoch 12/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.3757 - acc: 0.4774 - val_loss: 1.6335 - val_acc: 0.3359\n",
      "Epoch 13/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.3549 - acc: 0.4869 - val_loss: 1.6307 - val_acc: 0.3322\n",
      "Epoch 14/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.3221 - acc: 0.5016 - val_loss: 1.6635 - val_acc: 0.3301\n",
      "Epoch 15/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.2878 - acc: 0.5203 - val_loss: 1.6643 - val_acc: 0.3371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c66d2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "batch_size = 500\n",
    "\n",
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.47      0.46      0.46       744\n",
      "         sad       0.46      0.21      0.29       720\n",
      "     disgust       0.28      0.44      0.34       720\n",
      "       angry       0.32      0.40      0.35       736\n",
      "        fear       0.45      0.14      0.21       702\n",
      "    surprise       0.26      0.36      0.30       728\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      4350\n",
      "   macro avg       0.37      0.33      0.33      4350\n",
      "weighted avg       0.37      0.34      0.33      4350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "y_preds =  model.predict(X_test)\n",
    "y_preds =  np.argmax(y_preds, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "emolabels = []\n",
    "for e in emotions:\n",
    "  emolabels.append(e)\n",
    "\n",
    "print(classification_report(y_true, y_preds, target_names = emolabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\tsad\tdisgust\tangry\tfear\tsurprise\n",
      "[[0.08343195 0.1603057  0.37511113 0.19112545 0.10512042 0.08490526]\n",
      " [0.45274457 0.10900005 0.16382752 0.07221158 0.05826168 0.14395458]\n",
      " [0.07564437 0.31199667 0.20970939 0.07288248 0.24614432 0.08362284]\n",
      " [0.07446404 0.1160065  0.28984058 0.22279714 0.15530899 0.14158277]\n",
      " [0.07276982 0.08882906 0.18890159 0.1635094  0.30070078 0.18528935]\n",
      " [0.03486998 0.14478473 0.3069161  0.32656193 0.05823202 0.12863524]\n",
      " [0.5849222  0.07046012 0.05433699 0.04102251 0.0477002  0.20155795]\n",
      " [0.02707437 0.03696634 0.294535   0.4042312  0.05794214 0.1792509 ]]\n",
      "8.0\t16.0\t38.0\t19.0\t11.0\t8.0\n",
      "45.0\t11.0\t16.0\t7.0\t6.0\t14.0\n",
      "8.0\t31.0\t21.0\t7.0\t25.0\t8.0\n",
      "7.0\t12.0\t29.0\t22.0\t16.0\t14.0\n",
      "7.0\t9.0\t19.0\t16.0\t30.0\t19.0\n",
      "3.0\t14.0\t31.0\t33.0\t6.0\t13.0\n",
      "58.0\t7.0\t5.0\t4.0\t5.0\t20.0\n",
      "3.0\t4.0\t29.0\t40.0\t6.0\t18.0\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"まじきもい、あいつ\",\n",
    "    \"今日は楽しい一日だったよ\",\n",
    "    \"ペットが死んだ、実に悲しい\",\n",
    "    \"ふざけるな、死ね\",\n",
    "    \"ストーカー怖い\",\n",
    "    \"すごい！ほんとに！？\",\n",
    "    \"葉は植物の構成要素です。\",\n",
    "    \"ホームレスと囚人を集めて革命を起こしたい\"\n",
    "]\n",
    "import nlp\n",
    "for i, sen in enumerate(examples):\n",
    "    examples[i] = nlp.preprocess(sen)\n",
    "targets = preprocess(examples, tokenizer, maxlen=maxlen)\n",
    "print('\\t'.join(emolabels))\n",
    "for i, ds in enumerate(model.predict(targets)):\n",
    "    print('\\t'.join([str(round(100.0*d)) for d in ds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00b01e209a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memolabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
