{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17400, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "list = []\n",
    "size = 2900\n",
    "df = []\n",
    "\n",
    "emotions = [\"happy\", \"sad\", \"disgust\", \"angry\", \"fear\", \"surprise\"]\n",
    "\n",
    "for i, es in enumerate(emotions):\n",
    "  data = shuffle(pd.read_json(\"./data_processed/{}.json\".format(es))).iloc[:int(size)]\n",
    "  data['label'] = i\n",
    "  df.append(data)\n",
    "\n",
    "df = pd.concat(df)\n",
    "df = shuffle(df)\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "df.shape  #不是数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9787, 100)\n",
      "(3263, 100)\n",
      "(4350, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_features = 2000\n",
    "maxlen = 100\n",
    "\n",
    "# 特征化：\n",
    "# 1. text的特征化   Tokenizer()\n",
    "# 2. label的特征化  to_categorical()\n",
    "# label: 1 => [0,1,0,0,0,0]\n",
    "y = to_categorical(y)\n",
    "\n",
    "# 切割测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# 用keras的Tokenizer进行文本预处理，序列化，向量化等\n",
    "# 简单讲解Tokenizer如何实现转换。当我们创建了一个Tokenizer对象后，使用该对象的fit_on_texts()函数，可以将输入的文本中的每个词编号，编号是根据词频的，词频越大，编号越小。可能这时会有疑问：Tokenizer是如何判断文本的一个词呢？其实它是以空格去识别每个词。因为英文的词与词之间是以空格分隔，所以我们可以直接将文本作为函数的参数，但是当我们处理中文文本时，我们需要使用分词工具将词与词分开，并且词间使用空格分开。具体实现如下：\n",
    "def preprocess(data, tokenizer, maxlen = maxlen):\n",
    "    return(pad_sequences(tokenizer.texts_to_sequences(data), maxlen = maxlen))\n",
    "tokenizer = Tokenizer(num_words = max_features, char_level = True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = preprocess(X_train, tokenizer, maxlen)\n",
    "X_test = preprocess(X_test, tokenizer, maxlen)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)\n",
    "X_train.shape, X_val.shape, X_test.shape\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "# 使用Sequential模型来实现极为方便\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 150, input_length = maxlen))\n",
    "\n",
    "# Dropout 层还是非常重要的，值得一提。 Dropout 是一种方法能正则化我们的模型，防止出现过拟合。\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# 卷积过滤器的数量、每个卷积内核中的行数、每个卷积内核中的列数。\n",
    "model.add(Conv1D(32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "\n",
    "# MaxPooling2D 是一种减少模型中参数的方法，方式是在之前的层上滑动一个 2x2 的池化过滤器，在 2x2 过滤器中取 4 个值的最大值\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "\n",
    "# 卷积层(Conv Layer)\n",
    "model.add(Conv1D(64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "\n",
    "# 池化层(Pooling Layer)\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "\n",
    "# 最后再输出之前我们需要将上述几个层的输出作为全连接层的输入。由于卷积层和池化层的输出是2D的，因此需要将其压平，此时需要用到Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# 最后一层的输出大小为 6，对应了情感的 6 个类别。\n",
    "# 全连接层(Dense Layer)\n",
    "# 如果分类的类别过多地话则可以用softmax作为激活函数)\n",
    "model.add(Dense(6, activation = 'softmax'))\n",
    "\n",
    "# 现在只需编译模型，就可以训练它了。在我们编译模型时，会声明损失函数和优化器（SGD，Adam 等等）。\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9787 samples, validate on 3263 samples\n",
      "Epoch 1/15\n",
      "9787/9787 [==============================] - 23s 2ms/step - loss: 1.7771 - acc: 0.2039 - val_loss: 1.7628 - val_acc: 0.2200\n",
      "Epoch 2/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.7383 - acc: 0.2468 - val_loss: 1.7244 - val_acc: 0.2461\n",
      "Epoch 3/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.6850 - acc: 0.2912 - val_loss: 1.6903 - val_acc: 0.2682\n",
      "Epoch 4/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.6507 - acc: 0.3107 - val_loss: 1.6772 - val_acc: 0.2856\n",
      "Epoch 5/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.6187 - acc: 0.3283 - val_loss: 1.6708 - val_acc: 0.2908\n",
      "Epoch 6/15\n",
      "9787/9787 [==============================] - 19s 2ms/step - loss: 1.5897 - acc: 0.3565 - val_loss: 1.6867 - val_acc: 0.2911\n",
      "Epoch 7/15\n",
      "9787/9787 [==============================] - 19s 2ms/step - loss: 1.5536 - acc: 0.3770 - val_loss: 1.7393 - val_acc: 0.2721\n",
      "Epoch 8/15\n",
      "9787/9787 [==============================] - 19s 2ms/step - loss: 1.5290 - acc: 0.3938 - val_loss: 1.6853 - val_acc: 0.2979\n",
      "Epoch 9/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.5000 - acc: 0.4163 - val_loss: 1.6551 - val_acc: 0.3086\n",
      "Epoch 10/15\n",
      "9787/9787 [==============================] - 19s 2ms/step - loss: 1.4687 - acc: 0.4319 - val_loss: 1.6552 - val_acc: 0.3193\n",
      "Epoch 11/15\n",
      "9787/9787 [==============================] - 19s 2ms/step - loss: 1.4347 - acc: 0.4477 - val_loss: 1.6614 - val_acc: 0.3187\n",
      "Epoch 12/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.4080 - acc: 0.4744 - val_loss: 1.6350 - val_acc: 0.3279\n",
      "Epoch 13/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.3705 - acc: 0.4881 - val_loss: 1.7101 - val_acc: 0.3068\n",
      "Epoch 14/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.3428 - acc: 0.5004 - val_loss: 1.6606 - val_acc: 0.3399\n",
      "Epoch 15/15\n",
      "9787/9787 [==============================] - 20s 2ms/step - loss: 1.3040 - acc: 0.5230 - val_loss: 1.7072 - val_acc: 0.3255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a7458eef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "batch_size = 500\n",
    "\n",
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.51      0.41      0.46       735\n",
      "         sad       0.35      0.27      0.31       719\n",
      "     disgust       0.25      0.42      0.31       720\n",
      "       angry       0.31      0.43      0.36       716\n",
      "        fear       0.26      0.26      0.26       725\n",
      "    surprise       0.34      0.13      0.19       735\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      4350\n",
      "   macro avg       0.34      0.32      0.32      4350\n",
      "weighted avg       0.34      0.32      0.32      4350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "y_preds =  model.predict(X_test)\n",
    "y_preds =  np.argmax(y_preds, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "emolabels = []\n",
    "for e in emotions:\n",
    "  emolabels.append(e)\n",
    "\n",
    "print(classification_report(y_true, y_preds, target_names = emolabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\tsad\tdisgust\tangry\tfear\tsurprise\n",
      "1.0\t7.0\t29.0\t36.0\t14.0\t12.0\n",
      "42.0\t10.0\t19.0\t9.0\t8.0\t12.0\n",
      "2.0\t7.0\t13.0\t25.0\t48.0\t6.0\n",
      "3.0\t17.0\t28.0\t12.0\t32.0\t7.0\n",
      "1.0\t7.0\t29.0\t36.0\t14.0\t12.0\n",
      "1.0\t11.0\t30.0\t29.0\t24.0\t5.0\n",
      "3.0\t10.0\t31.0\t23.0\t24.0\t9.0\n",
      "6.0\t13.0\t24.0\t34.0\t10.0\t12.0\n",
      "64.0\t8.0\t5.0\t2.0\t5.0\t17.0\n",
      "5.0\t13.0\t15.0\t32.0\t14.0\t20.0\n",
      "2.0\t11.0\t7.0\t27.0\t30.0\t22.0\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"正気とは到底思えない先ずは制裁を課してほしいです根本的には国交を縮小するなど見直しが必要だと思う\",\n",
    "    \"今日は楽しい一日だったよ\",\n",
    "    \"戦争になった場合、サムスン産のスマホが大活躍するでしょう。自然爆発するあの兵器は最強です。ガチだと、日本には勝てないでしょう。日本の兵器が優秀ではなく、韓国の兵器がショボ過ぎるからです。\",\n",
    "    \"ペットが死んだ、実に悲しい\",\n",
    "    \"正気とは到底思えない先ずは制裁を課してほしいです根本的には国交を縮小するなど見直しが必要だと思う\",\n",
    "    \"ふざけるな、死ね\",\n",
    "    \"ストーカー怖い\",\n",
    "    \"すごい！ほんとに！？\",\n",
    "    \"葉は植物の構成要素です。\",\n",
    "    \"ホームレスと囚人を集めて革命を起こしたい\",\n",
    "    \"いずれ、未だに真相が怪しくて仕方ない「３．１独立運動」とやらが全く別の新しい「３．１独立戦争」とやらに昇華するんだろうね…。そして、新たな英雄や誇り高き事件等々が次々に生まれるとともに、日々のニュース報道や政府報道で、まさに今起こっているかの様に、報道・拡散されて、更なる反日思想を埋め込んで行くんだろうね…。\"\n",
    "]\n",
    "import nlp\n",
    "for i, sen in enumerate(examples):\n",
    "    examples[i] = nlp.preprocess(sen)\n",
    "targets = preprocess(examples, tokenizer, maxlen=maxlen)\n",
    "print('\\t'.join(emolabels))\n",
    "for i, ds in enumerate(model.predict(targets)):\n",
    "    print('\\t'.join([str(round(100.0*d)) for d in ds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
